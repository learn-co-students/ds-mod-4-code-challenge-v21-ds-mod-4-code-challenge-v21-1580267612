{"cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Module 4 Code Challenge"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This code challenge is designed to test your understanding of the Module 4 material. It covers:\n", "\n", "* Clustering\n", "* Time Series\n", "* Natural Language Processing\n", "* Neural Networks\n", "\n", "_Read the instructions carefully._ You will be asked both to write code and respond to a few short answer questions.\n", "\n", "The goal here is to demonstrate your knowledge. Showing that you know things about certain concepts is more important than getting the best model. You can use any libraries you want to solve the problems in the assessment. \n", "\n", "You will have up to 90 minutes to complete this assessments\n", "### Note on the short answer questions\n", "\n", "For the short answer questions, _please use your own words._ The expectation is that you have **not** copied and pasted from an external source, even if you consult another source to help craft your response. While the short answer questions are not necessarily being assessed on grammatical correctness or sentence structure, you should do your best to communicate yourself clearly."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: Clustering [Suggested Time: 25 min]\n", "\n", "### Clustering Algorithms: k-means and hierarchical agglomerative clustering\n", "\n", "#### 1.1) Using the gif below for reference, describe the steps of the k-means clustering algorithm.\n", "* If the gif doesn't run, you may access it via [this link](images/centroid.gif).\n", "\n", "<img src='images/centroid.gif'>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your answer here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 1.2) In a similar way, describe the process behind Hierarchical Agglomerative Clustering."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your answer here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### k-means Clustering\n", "\n", "For this question, you will apply k-means clustering to your now friend, the wine dataset. \n", "\n", "You will use scikit-learn to fit k-means clustering models, and you will determine the optimal number of clusters to use by looking at silhouette scores. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["We load the wine dataset for you in the cell below. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_wine\n", "\n", "X, y = load_wine(return_X_y=True)\n", "wine = load_wine()\n", "X = pd.DataFrame(X, columns = wine.feature_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**1.3) Write a function called `get_labels` that will find `k` clusters in a dataset of features `X`, and return the labels for each row of `X`.**\n", "\n", "_Hint: Within the function, you'll need to:_\n", "* instantiate a k-means clustering model (use `random_state = 1` for reproducibility),\n", "* fit the model to the data, and\n", "* return the labels for each point."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None and pass with appropriate code\n", "def get_labels(k, X):\n", "    \n", "    # Instantiate a k-means clustering model with random_state=1 and n_clusters=k\n", "    kmeans = None\n", "    \n", "    # Fit the model to the data\n", "    None\n", "    \n", "    # return the predicted labels for each row in the data\n", "    pass "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**1.4) Fit the k-means algorithm to the wine data for k values in the range 2 to 9 using the function you've written above. Obtain the silhouette scores for each trained k-means clustering model, and place the values in a list called `silhouette_scores`.** \n", "\n", "We have provided you with some starter code in the cell below.\n", "\n", "_Hints: What imports do you need? Do you need to pre-process the data in any way before fitting the k-means clustering algorithm?_ "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "\n", "silhouette_scores= []\n", "\n", "for k in range(2, 10):\n", "    labels = None \n", "    \n", "    score = silhouette_score(None, None, metric='euclidean')\n", "    \n", "    silhouette_scores.append(score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run the cell below to plot the silhouette scores obtained for each different value of k against k, the number of clusters we asked the algorithm to find. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(range(2, 10), silhouette_scores, marker='o')\n", "plt.title('Silhouette scores vs number of clusters')\n", "plt.xlabel('k (number of clusters)')\n", "plt.ylabel('silhouette score')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**1.5) Which value of k would you choose based on the plot of silhouette scores? How does this number compare to the number of classes in the wine dataset?**\n", "\n", "Hint: this number should be <= 5.  If it's not, check your answer in the previous section."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your answer here "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Time Series [Suggested Time: 15 minutes]\n", "\n", "<!---Create stock_df and save as .pkl\n", "stocks_df = pd.read_csv(\"raw_data/all_stocks_5yr.csv\")\n", "stocks_df[\"clean_date\"] = pd.to_datetime(stocks_df[\"date\"], format=\"%Y-%m-%d\")\n", "stocks_df.drop([\"date\", \"clean_date\", \"volume\", \"Name\"], axis=1, inplace=True)\n", "stocks_df.rename(columns={\"string_date\": \"date\"}, inplace=True)\n", "pickle.dump(stocks_df, open(\"write_data/all_stocks_5yr.pkl\", \"wb\"))\n", "--->"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["import pickle\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "from pandas.plotting import register_matplotlib_converters\n", "register_matplotlib_converters()"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>open</th>\n", "      <th>high</th>\n", "      <th>low</th>\n", "      <th>close</th>\n", "      <th>date</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>15.07</td>\n", "      <td>15.12</td>\n", "      <td>14.63</td>\n", "      <td>14.75</td>\n", "      <td>February 08, 2013</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>14.89</td>\n", "      <td>15.01</td>\n", "      <td>14.26</td>\n", "      <td>14.46</td>\n", "      <td>February 11, 2013</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>14.45</td>\n", "      <td>14.51</td>\n", "      <td>14.10</td>\n", "      <td>14.27</td>\n", "      <td>February 12, 2013</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>14.30</td>\n", "      <td>14.94</td>\n", "      <td>14.25</td>\n", "      <td>14.66</td>\n", "      <td>February 13, 2013</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>14.94</td>\n", "      <td>14.96</td>\n", "      <td>13.16</td>\n", "      <td>13.99</td>\n", "      <td>February 14, 2013</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["    open   high    low  close               date\n", "0  15.07  15.12  14.63  14.75  February 08, 2013\n", "1  14.89  15.01  14.26  14.46  February 11, 2013\n", "2  14.45  14.51  14.10  14.27  February 12, 2013\n", "3  14.30  14.94  14.25  14.66  February 13, 2013\n", "4  14.94  14.96  13.16  13.99  February 14, 2013"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["stocks_df = pickle.load(open(\"write_data/all_stocks_5yr.pkl\", \"rb\"))\n", "stocks_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1) Transform the `date` feature so that it becomes a `datetime` object that contains the following format: YYYY-MM-DD and set `date` to be the index of `stocks_df`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2) Perform downsampling `stocks_df` that takes the mean of the `open`, `high`, `low`, and `close` features on a monthly basis. Store the results in `stocks_monthly_df`.\n", "\n", "> Hint: `stocks_monthly_df` should have 61 rows and 4 columns after you perform downsampling."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stocks_monthly_df.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.3) Create a line graph that visualizes the monthly open stock prices from `stocks_monthly_df` for the purposes of identifying if average monthly open stock price is stationary or not using the rolling mean and rolling standard deviation.\n", "\n", "> Hint: \n", "> * store your sliced version of `stocks_monthly_df` in a new DataFrame called `open_monthly_df`;\n", "> * use a window size of 3 to represent one quarter of time in a year"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "\n", "open_monthly_df = None\n", "\n", "rolmean = None\n", "rolstd = None\n", "\n", "# note: do not rename the variables otherwise the plot code will not work"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(13, 10))\n", "ax.plot(open_monthly_df, color=\"blue\",label=\"Average monthly opening stock price\")\n", "ax.plot(rolmean, color=\"red\", label=\"Rolling quarterly mean\")\n", "ax.plot(rolstd, color=\"black\", label=\"Rolling quarterly std. deviation\")\n", "ax.set_ylim(0, 120)\n", "ax.legend()\n", "fig.suptitle(\"Average monthly open stock prices, Feb. 2013 to Feb. 2018\")\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Based on your visual inspection of the graph, is the monthly open stock price stationary?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your written answer here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.4) Use the Dickey-Fuller Test to identify if `open_monthly_df` is stationary"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Does this confirm your answer from Question 3? Explain why the time series is stationary or not based on the output from the Dickey-Fuller Test."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your answer here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.5) Looking at the decomposition of the time series in `open_monthly_df`, it looks like the peaks are the same value. To confirm or deny this, create a function that returns a dictionary where each key is year and each value is the maximum value from the `seasonal` object for each year."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from statsmodels.tsa.seasonal import seasonal_decompose\n", "decomposition = seasonal_decompose(np.log(open_monthly_df))\n", "\n", "# Gather the trend, seasonality and noise of decomposed object\n", "seasonal = decomposition.seasonal\n", "\n", "# Plot gathered statistics\n", "plt.figure(figsize=(13, 10))\n", "plt.plot(seasonal,label='Seasonality', color=\"blue\")\n", "plt.title(\"Seasonality of average monthly open stock prices, Feb. 2013 to Feb. 2018\")\n", "plt.ylabel(\"Average monthly open stock prices\")\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calc_yearly_max(seasonal_series):\n", "    \"\"\"Returns the max seasonal value for each year\"\"\"\n", "    # Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["calc_yearly_max(seasonal)"]}, {"cell_type": "markdown", "metadata": {"heading_collapsed": true}, "source": ["## Part 3: Natural Language Processing [Suggested Time: 20 minutes]"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["In this exercise we will attempt to classify text messages as \"SPAM\" or \"HAM\" using TF-IDF Vectorization. Once we successfully classify our texts we will examine our results to see which words are most important to each class of text messages. \n", "\n", "Complete the functions below and answer the question(s) at the end. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["#import necessary libraries \n", "import pandas as pd\n", "import numpy as np\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.metrics import accuracy_score, confusion_matrix\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.model_selection import train_test_split\n", "import string\n", "from nltk.corpus import stopwords\n", "from nltk import word_tokenize"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["#read in data\n", "df_messages = pd.read_csv('data/spam.csv', usecols=[0,1])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["#convert string labels to 1 or 0 \n", "le = LabelEncoder()\n", "df_messages['target'] = le.fit_transform(df_messages['v1'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["#examine or data\n", "df_messages.head()"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["### TF-IDF"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["#separate features and labels \n", "X = df_messages['v2']\n", "y = df_messages['target']\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["#generate a list of stopwords \n", "stopwords_list = stopwords.words('english') + list(string.punctuation)\n"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["<b>1) Let's create a function that takes in our various texts along with their respective labels and uses TF-IDF to vectorize the texts.  Recall that TF-IDF helps us \"vectorize\" text (turn text into numbers) so we can do \"math\" with it.  It is used to reflect how relevant a term is in a given document in a numerical way. </b>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["#generate tf-idf vectorization (use sklearn's TfidfVectorizer) for our data\n", "def tfidf(X, y,  stopwords_list): \n", "    '''\n", "    Generate train and test TF-IDF vectorization for our data set\n", "    \n", "    Parameters\n", "    ----------\n", "    X: pandas.Series object\n", "        Pandas series of text documents to classify \n", "    y : pandas.Series object\n", "        Pandas series containing label for each document\n", "    stopwords_list: list ojbect\n", "        List containing words and punctuation to remove. \n", "    Returns\n", "    --------\n", "    tf_idf_train :  sparse matrix, [n_train_samples, n_features]\n", "        Vector representation of train data\n", "    tf_idf_test :  sparse matrix, [n_test_samples, n_features]\n", "        Vector representation of test data\n", "    y_train : array-like object\n", "        labels for training data\n", "    y_test : array-like object\n", "        labels for testing data\n", "    vectorizer : vectorizer object\n", "        fit TF-IDF vecotrizer object\n", "\n", "    '''\n", "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "    \n", "    pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["tf_idf_train, tf_idf_test, y_train, y_test, vecotorizer = tfidf(X, y, stopwords_list)"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["### Classification"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["<b>2) Now that we have a set of vectorized training data we can use this data to train a classifier to learn how to classify a specific text based on the vectorized version of the text. Below we have initialized a simple Naive Bayes Classifier and Random Forest Classifier. Complete the function below which will accept a classifier object, a vectorized training set, vectorized test set, and list of training labels and return a list of predictions for our training set and a separate list of predictions for our test set.</b> "]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["nb_classifier = MultinomialNB()\n", "rf_classifier = RandomForestClassifier(n_estimators=100)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["#create a function that takes in a classifier and trains it on our tf-idf vectors and generates test and train predictiions\n", "def classify_text(classifier, tf_idf_train, tf_idf_test, y_train):\n", "    '''\n", "    Train a classifier to identify whether a message is spam or ham\n", "    \n", "    Parameters\n", "    ----------\n", "    classifier: sklearn classifier\n", "       initialized sklearn classifier (MultinomialNB, RandomForestClassifier, etc.)\n", "    tf_idf_train : sparse matrix, [n_train_samples, n_features]\n", "        TF-IDF vectorization of train data\n", "    tf_idf_test : sparse matrix, [n_test_samples, n_features]\n", "        TF-IDF vectorization of test data\n", "    y_train : pandas.Series object\n", "        Pandas series containing label for each document in the train set\n", "    Returns\n", "    --------\n", "    train_preds :  list object\n", "        Predictions for train data\n", "    test_preds :  list object\n", "        Predictions for test data\n", "    '''\n", "    #fit the classifier with our training data\n", "    \n", "    #predict the labels of our train data and store them in train_preds\n", "    \n", "    #predict the labels of our test data and store them in test_preds\n", "    pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["#generate predictions for Naive Bayes Classifier\n", "nb_train_preds, nb_test_preds = classify_text(nb_classifier,tf_idf_train, tf_idf_test, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true, "scrolled": false}, "outputs": [], "source": ["print(confusion_matrix(y_test, nb_test_preds))\n", "print(accuracy_score(y_test, nb_test_preds))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true, "scrolled": true}, "outputs": [], "source": ["#generate predictions for Random Forest Classifier\n", "rf_train_preds, rf_test_preds = classify_text(rf_classifier,tf_idf_train, tf_idf_test, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true, "scrolled": false}, "outputs": [], "source": ["print(confusion_matrix(y_test, rf_test_preds))\n", "print(accuracy_score(y_test, rf_test_preds))"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["You can see both classifiers do a pretty good job classifying texts as either \"SPAM\" or \"HAM\". Let's figure out which words are the most important to each class of texts! Recall that Inverse Document Frequency can help us determine which words are most important in an entire corpus or group of documents. \n", "\n", "<b>3) Create a function that calculates the IDF of each word in our collection of texts.</b>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["def get_idf(class_, df, stopwords_list):\n", "    '''\n", "    Get ten words with lowest IDF values representing 10 most important\n", "    words for a defined class (spam or ham)\n", "    \n", "    Parameters\n", "    ----------\n", "    class_ : str object\n", "        string defining class 'spam' or 'ham'\n", "    df : pandas DataFrame object\n", "        data frame containing texts and labels\n", "    stopwords_list: list object\n", "        List containing words and punctuation to remove. \n", "    --------\n", "    important_10 : pandas dataframe object\n", "        Dataframe containing 10 words and respective IDF values\n", "        representing the 10 most important words found in the texts\n", "        associated with the defined class\n", "    '''\n", "    #generate series containing all texts associated with the defined class\n", "    docs = 'code here'\n", "    \n", "    #initialize dictionary to count document frequency \n", "    # (number of documents that contain a certain word)\n", "    class_dict = {}\n", "    \n", "    #loop over each text and split each text into a list of its unique words \n", "    for doc in docs:\n", "        words = set(doc.split())\n", "        \n", "        #loop over each word and if it is not in the stopwords_list add the word \n", "        #to class_dict with a value of 1. if it is already in the dictionary\n", "        #increment it by 1\n", "        \n", "    #take our dictionary and calculate the \n", "    #IDF (number of docs / number of docs containing each word) \n", "    #for each word and return the 10 words with the lowest IDF \n", "    pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true, "scrolled": true}, "outputs": [], "source": ["get_idf('spam', df_messages, stopwords_list)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hidden": true}, "outputs": [], "source": ["get_idf('ham', df_messages, stopwords_list)"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["### Explain\n", "<b> 4) Imagine that the word \"school\" has the highest TF-IDF value in the second document of our test data. What does that tell us about the word school? </b>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your answer here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Regularization and Optimization of Neural Networks [Suggested Time: 20 minutes]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now you're going to train full neural networks on a _small_ set of data. It is a binary classification problem in which you need to identify whether or not a dot will belong to the teal or orange class."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(0)\n", "# generate 2d classification dataset\n", "X, y = make_circles(n_samples=450, noise=0.12)\n", "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n", "colors = {0:'teal', 1:'orange'}\n", "fig, ax = pyplot.subplots()\n", "grouped = df.groupby('label')\n", "for key, group in grouped:\n", "    if key != 2:\n", "        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n", "pyplot.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the two cells below, the set of data has been split into a training and testing set and then fit to a neural network with two hidden layers. Run the two cells below to see how well the model performs."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#train/test/split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(0)\n", "\n", "#Instantiate Classifier\n", "classifier = Sequential()\n", "\n", "#Hidden Layer\n", "classifier.add(Dense(\n", "    32, \n", "    activation='relu', \n", "    input_dim=2,\n", "    kernel_initializer='random_normal',\n", "\n", "))\n", "\n", "#Hidden Layer\n", "classifier.add(Dense(\n", "    32,\n", "    activation='relu', \n", "    input_dim=2,\n", "    kernel_initializer='random_normal',\n", "\n", "))\n", "\n", "#Output Layer\n", "classifier.add(Dense(\n", "    1, \n", "    activation='sigmoid',\n", "    kernel_initializer='random_uniform',\n", "))\n", "\n", "classifier.compile(optimizer ='adam',loss=\"binary_crossentropy\",metrics =['accuracy'])\n", "\n", "classifier.fit(X_train, y_train, epochs=25, verbose=0, batch_size=10, shuffle=False)\n", "\n", "# TRAIN\n", "\n", "#predict classes\n", "predicted_vals_train = classifier.predict_classes(X_train)\n", "#show accuracy score\n", "print(accuracy_score(y_train,predicted_vals_train))\n", "\n", "\n", "# TEST\n", "\n", "#predict classess\n", "predicted_vals_test = classifier.predict_classes(X_test)\n", "#show accuracy score\n", "print(accuracy_score(y_test,predicted_vals_test))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### 1) Modify the code below to use L2 regularization\n", "\n", "\n", "The model appears to be overfitting. To deal with this overfitting, modify the code below to include L2 regularization in the model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(0)\n", "\n", "#Instantiate Classifier\n", "classifier2 = Sequential()\n", "\n", "#Hidden Layer\n", "classifier2.add(Dense(\n", "    32, \n", "    activation='relu', \n", "    input_dim=2,\n", "    kernel_initializer='random_normal'\n", "\n", "))\n", "\n", "#Hidden Layer\n", "classifier2.add(Dense(\n", "    32,\n", "    activation='relu', \n", "    input_dim=2,\n", "    kernel_initializer='random_normal'\n", "\n", "))\n", "\n", "#Output Layer\n", "classifier2.add(Dense(\n", "    1, \n", "    activation='sigmoid',\n", "    kernel_initializer='random_uniform',\n", "))\n", "\n", "classifier2.compile(optimizer ='adam',loss=\"binary_crossentropy\",metrics =['accuracy'])\n", "\n", "classifier2.fit(X_train, y_train, epochs=25, verbose=0, batch_size=10, shuffle=False)\n", "\n", "# TRAIN\n", "\n", "#predict classes\n", "predicted_vals_train = classifier2.predict_classes(X_train)\n", "#show accuracy score\n", "print(accuracy_score(y_train,predicted_vals_train))\n", "\n", "# TEST\n", "\n", "#predict classess\n", "predicted_vals_test = classifier2.predict_classes(X_test)\n", "#show accuracy score\n", "print(accuracy_score(y_test,predicted_vals_test))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Did the regularization you performed prevent overfitting?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your answer here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### 2) Explain how regularization is related to the bias/variance tradeoff within Neural Networks and how it's related to the results you just achieved in the training and test accuracies of the previous models. What does regularization change in the training process (be specific to what is being regularized and how it is regularizing)?\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your answer here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### 3) How might L1  and dropout regularization change a neural network's architecture?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your answer here"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}}, "nbformat": 4, "nbformat_minor": 2}